package NYT.NScheduler.NProto;

import "yt/ytlib/chunk_holder/chunk.proto";
import "yt/ytlib/table_client/table_reader.proto";
import "yt/ytlib/table_client/table_chunk_meta.proto";
import "yt/ytlib/file_server/file_ypath.proto";
import "yt/ytlib/misc/error.proto";
import "yt/ytlib/misc/guid.proto";

////////////////////////////////////////////////////////////////////////////////

// Describes a part of input table(s) to be processed by a job.
message TTableInputSpec
{
    // Chunks comprising the input.
    repeated NYT.NTableClient.NProto.TInputChunk chunks = 1;
}

// Defines how to store output from a job into a table.
message TTableOutputSpec
{
    // YSON-serialized channels that must be used for writing the table.
    required bytes channels = 1;

    // The chunk list where output chunks must be placed.
    required NYT.NProto.TGuid chunk_list_id = 2;

    // If non-empty then the output is expected to be sorted accordingly.
    repeated string key_columns = 3;
}

// Sent from the scheduler to an exec node.
// Describes a portion of processing for a job.
message TJobSpec
{
    required int32 type = 1;

    // Configuration for IO during job execution.    
    required bytes io_config = 2;

    // The transaction used for writing output chunks.
    required NYT.NProto.TGuid output_transaction_id = 3;

    // Job input.
    repeated TTableInputSpec input_specs = 4;

    // Job output.
    repeated TTableOutputSpec output_specs = 5;

    extensions 100 to max;
}

// Typically sent from a job proxy to its supervisor
// (but is also passed on to other parts of the system).
// Describes the outcome of the job, in particular if it has finished successfully.
message TJobResult
{
    required NYT.NProto.TError error = 1;

    // List of chunks the job was unable to read.
    repeated NYT.NProto.TGuid failed_chunk_ids = 2;

    extensions 100 to max;
}

////////////////////////////////////////////////////////////////////////////////

// Specification for starting user code during a job.
message TUserJobSpec
{
    // Additional files to be placed into the sandbox.
    repeated NYT.NFileServer.NProto.TRspFetch files = 1;

    // The user command to be executed.
    required string shell_command = 2;

    // Input format description (in YSON).
    required string input_format = 3;

    // Output format description (in YSON).
    required string output_format = 4;
}

// User code result.
message TUserJobResult
{
    optional NYT.NProto.TGuid stderr_chunk_id = 1;
}

////////////////////////////////////////////////////////////////////////////////

// Map jobs.
/*
 * Conceptually map is the simplest operation.
 * Input consists of a number of tables (or parts thereof).
 * These tables are merged together into a sequence of rows,
 * sequence is split into fragments and these fragments
 * are fed to jobs. Each job runs a given shell command.
 * The outputs of jobs are collected thus forming a number
 * of output tables.
 *
 * The input spec should contain TMapJobSpecExt.
 * The result must contain TMapJobResultExt.
 *
 */

message TMapJobSpecExt
{
    extend TJobSpec
    {
        optional TMapJobSpecExt map_job_spec_ext = 100;
    }

    required TUserJobSpec mapper_spec = 1;
}

message TMapJobResultExt
{
    extend TJobResult
    {
        optional TMapJobResultExt map_job_result_ext = 100;
    }

    required TUserJobResult mapper_result = 1;
}

////////////////////////////////////////////////////////////////////////////////

// Merge jobs.
/*
 * A merge job takes a number of chunks sequences (each containing sorted data)
 * and merges them. The result is split into chunks again.
 *
 * The input spec should contain TMergeJobSpecExt.
 *
 */

message TMergeJobSpecExt
{
    extend TJobSpec
    {
        optional TMergeJobSpecExt merge_job_spec_ext = 101;
    }

    // For EJobType::SortedMerge, contains columns used for comparison.
    repeated string key_columns = 1;
}

////////////////////////////////////////////////////////////////////////////////

// Partition jobs.
/*
 * A partition jobs read the input and scatters the rows into buckets depending
 * on their keys. When a bucket becomes full, it is written as a block.
 * Output blocks are marked with |partition_tag| to enable subsequently
 * started sort jobs to fetch appropriate portions of data.
 *
 * The input spec should contain TPartitionJobSpecExt.
 * The result must contain TPartitionJobResultExt.
 *
 */

message TPartitionJobSpecExt
{
    extend TJobSpec
    {
        optional TPartitionJobSpecExt partition_job_spec_ext = 103;
    }

    repeated NYT.NTableClient.NProto.TKey partition_keys = 3;
    repeated string key_columns = 5;
}

message TPartitionJobResultExt
{
    extend TJobResult
    {
        optional TPartitionJobResultExt partition_job_result_ext = 102;
    }

    repeated NYT.NTableClient.NProto.TInputChunk chunks = 1;
}

////////////////////////////////////////////////////////////////////////////////

// Sort jobs.
/*
 * A sort job reads the input chunks, sorts the rows, and then flushes
 * the rows into a sequence of output chunks.
 *
 * The input spec should contain TSortJobSpecExt.
 * The result must contain TSortJobResultExt.
 *
 */
message TSortJobSpecExt
{
    extend TJobSpec
    {
        optional TSortJobSpecExt sort_job_spec_ext = 104;
    }

    optional int32 partition_tag = 3 [default = -1];
    repeated string key_columns = 5;
}

message TSortJobResultExt
{
    extend TJobResult
    {
        optional TSortJobResultExt sort_job_result_ext = 103;
    }

    repeated NYT.NTableClient.NProto.TInputChunk chunks = 1;
}

////////////////////////////////////////////////////////////////////////////////

// Reduce jobs.
/*
 * "Everthing is either a sort or a merge. Reduce is the latter." (c) Pavel Sushin
 *
 * The input spec should contain TReduceJobSpecExt.
 * The result must contain TReduceJobResultExt.
 *
 */

message TReduceJobSpecExt
{
    extend TJobSpec
    {
        optional TReduceJobSpecExt reduce_job_spec_ext = 105;
    }

    repeated string key_columns = 1;
    required TUserJobSpec reducer_spec = 2;
}

message TReduceJobResultExt
{
    extend TJobResult
    {
        optional TReduceJobResultExt reduce_job_result_ext = 104;
    }

    // Boundary keys for each sorted output table.
    // Used by the scheduler to reorder the chunks.

    message TOutputBoundaryKeys
    {
        required int32 table_index = 1;
        required NYT.NTableClient.NProto.TKey start = 2;
        required NYT.NTableClient.NProto.TKey end = 3;
    }

    repeated TOutputBoundaryKeys boundary_keys = 1;
    required TUserJobResult reducer_result = 2;
}

////////////////////////////////////////////////////////////////////////////////
